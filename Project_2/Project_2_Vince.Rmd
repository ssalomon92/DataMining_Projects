---
output:
  html_document: default
  word_document: default
---


## Project 2
#### Group Members: Vince Miller & Sebastian Salomon

```{r setup, include=FALSE}
# create tool kit and visualization sizes
knitr::opts_chunk$set(fig.height=3.5,fig.width=8)
knitr::opts_chunk$set(echo = FALSE)
library('MASS')
library('dplyr')
library('knitr')
library('lemon')
library('tweedie')
library('kableExtra')
library('modeest')
library('ggplot2')
library('tidyr')
library('reshape')
library('egg')
library('gridExtra')
library('cowplot')
library('sqldf')
library('fpc')
library('akmeans')
library('stylo')
```


```{r echo=FALSE}
# Set working directory for 
setwd('C:\\Users\\MillerV\\Documents\\Masters Statistics\\CSE7331\\Projects\\DataMining_Projects')
## read in all the data and convert to tibble, this is the tidyverse dataframe
df.claims <- read.csv(file="Claims.csv") %>% as_tibble()
df.members <- read.csv(file="Members.csv") %>% as_tibble()
df.drugs <- read.csv(file="DrugCount.csv") %>% as_tibble()
df.labs <- read.csv(file="LabCount.csv") %>% as_tibble()
```
### 3. Data Preparation [30 points]
* Describe which features you want to use for clustering and why. [20]

```{r, echo=F}
# filter for year 1
df.claims_Y1 <- df.claims %>%
  filter(Year=='Y1') %>%
  select(-Year)

df.labs_Y1 <- df.labs %>%
  filter(Year=='Y1') %>%
  select(-Year)

df.drugs_Y1 <- df.drugs %>%
  filter(Year=='Y1') %>%
  select(-Year)
  
df.members_Y1 <- df.members

# free memory by removing unfiltered data
rm(df.claims,df.drugs,df.labs)

# create number of claims, providers, specialists, vendors, place service and pcp variables
df.members_Y1 <- df.claims_Y1 %>%
  group_by(MemberID) %>%
  summarize(N_claims = n(),
            N_Specialists=n_distinct(Specialty), 
            N_Providers=n_distinct(ProviderID),
            N_vendors=n_distinct(Vendor),
            N_PCP=n_distinct(PCP),
            N_places=n_distinct(PlaceSvc)) %>%
  left_join(df.members_Y1, by='MemberID')

# create number of drug claims variables assume 0 drug claims for member ID not in year 1 of drug dataset.
df.members_Y1 <- df.drugs_Y1 %>%
  group_by(MemberID) %>%
  summarize(N_drugclaims = n()) %>%
  right_join(df.members_Y1, by='MemberID') %>%
  mutate(N_drugclaims = replace_na(N_drugclaims,replace=0))

# create number of lab claims variables assume 0 drug claims for member ID not in year 1 of lab dataset.
df.members_Y1 <- df.labs_Y1 %>%
  group_by(MemberID) %>%
  summarize(N_labclaims = n()) %>%
  right_join(df.members_Y1, by='MemberID') %>%
  mutate(N_labclaims = replace_na(N_labclaims,replace=0))

# recode charleston index and create difference variable
df.members_Y1 <- df.claims_Y1 %>%
  mutate(CharlsonIndex = recode(CharlsonIndex, `0`=0, `1-2`=1.5, `3-4`=3.5, `5+`=5)) %>%
  group_by(MemberID) %>%
  summarize(Max_Charlson = max(CharlsonIndex),
            Min_Charlson = min(CharlsonIndex)) %>%
  mutate(char_diff = Max_Charlson - Min_Charlson) %>%
  right_join(df.members_Y1, by='MemberID')

# translate age variable and change sex to numeric
df.members_Y1 <- df.members_Y1 %>%
  mutate(age = recode(AgeAtFirstClaim, `""`=NULL, `0-9`=4.5, `10-19`=14.5, `20-29`=24.5, `30-39`=34.5, `40-49`=44.5, `50-59`=54.5, `60-69`= 64.5, `70-79`=74.5, `80+`=84.5),
         male = ifelse(Sex=='M',1,0),
         female= ifelse(Sex=='F',1,0)) %>%
  select(-Sex,-AgeAtFirstClaim)
```

```{r, echo=F}
# create table for viewing variables
df.created_features <- cbind(lapply(df.members_Y1, max,na.rm=T),lapply(df.members_Y1, mean, na.rm=T),lapply(df.members_Y1, min, na.rm=T),lapply(df.members_Y1,function(x) sum(is.na(x))))

# change col names and round
colnames(df.created_features) <- c('MAX','MEAN','MIN', 'NA Count')
df.created_features[2:15,] <- format(df.created_features[2:15,], digits=3)

# create table
kable(df.created_features[2:15,]) %>%
  group_rows('Charlson Features',1,3) %>% 
  group_rows('Counts',4,11) %>%
  group_rows('Age',12,12) %>% 
  group_rows('Gender',13,14) %>%
  kable_styling(full_width=F)

```
Table 1: Features used for clustering

#### Discussion
All features for clustering were created at the member level. This was done to help understand how we can cluster patients as opposed to other possible levels such as claim level. 

The first 3 variables shown are the min, max, and difference for the charlson index for each of the patients claims. These variables can be used at an attempt to use severity of a members claim or the change of severity from claim to claim for a member.

Count variables were also created at the member level. The total number of claims by a member. The number of lab claims and drug claims each member made. The number of different specialist a member has seen. The number of unique providers, vendors, and pcp that each member has used for a claim as well as the number of different places. These count variables can be used to determine if patterns of activity can cluster our members.

The age of the members has been included. This variable can be used for controlling for age while clustering. Also it should be noted that Age has missing values. Our team chose to impute the values of age with the mean age for the entire members dataset. This could cause issues with our analysis because both kmeans and hierarchical clustering require each observation to be in a cluster. This imputation method could result in the clustering algorithm performance to degrage. Due to the reasons stated age will not be used for clustering but only for investigating the clusters. 

The gender of the member is also inclduded. This variable has been split into two columns; an indicator for male and an indicator for female. There were missing values therefore it is possible to have a 0 for each of these columns.

* What is the scale of measurement of the features and what are appropriate distance measures?[10]

```{r}
# standardize the data
df.members_Y1_scaled <- df.members_Y1 %>% 
  mutate_at(vars(-MemberID), funs(scale(.) %>% as.vector))
```

#### Discussion
The features used will be scaled to have a mean of 0 and a standard deviation of 1 for both kmeans and hierarchical clustering. Euclidean distance as opposed to cosine distance or a correlation measure will be used for each of the stated clustering methods. Currently, we do not have evidence to suggest one option of distance is better as opposed to others for our features. Additional analysis could be performed to compare how the clusters are formed using cosine distance or correlation measure, but that is not within the scope of our analysis. Clustering algorithms require subjective decisions such as distance measure choice.There is no doubt that differnt choices result in different clustering results, but due to the exploratory nature of clustering the ability to generalize intuitive results is a acceptable reasoning for selecting a distance.

### 4. Modeling [50 points]
* Perform cluster analysis using several methods (at least k-means and hierarchical clustering)
using different feature subsets. [30]

```{r, echo=F, warning=F}
# so the clusters are the same
set.seed(33)

# remove Member ID
df <- df.members_Y1_scaled %>% select(-MemberID)

# fill age
df <- df %>%
  mutate(age = replace_na(age, replace=0))

# create df for kmeans
# only include the count variables
df.kmeans <- df %>% select(N_labclaims,N_drugclaims,N_claims,N_Specialists,N_Providers,N_PCP,N_vendors,N_places)

# cluster
out.withinss <- rep(0,15)

for(i in seq(1,15,1)){
  out.withinss[i] <- kmeans(df.kmeans,i,nstart=20)$tot.withinss
  }

# plot the objective function kmeas attempts to minimize
plot(x=seq(1,15,1), y=out.withinss ,type="l")
```

#### Discussion
Appears the objective function slope decreases after K=4. Our kmeans clustering analysis will examine 4 clusters.

```{r}
# from the plot above it appears at k=4 and k=5 there the slope decrease
km.out <- kmeans(df.kmeans,4,nstart=20)

def.par <- par(no.readonly = TRUE, mar = c(8.1, 4.1, 4.1, 2.1)) # save default, for resetting...
layout(t(1:4)) # 4 plots in one
for(i in 1:4) barplot(km.out$centers[i,], main=paste("Cluster", i),las=2, ylim=c(-2,4.5))
```

#### Discussion
Alot of differences in clusters...


```{r}
cluster.count <- km.out$size %>% as_tibble() %>%
  mutate(Cluster = c(1,2,3,4)) %>%
  dplyr::rename(Count=value)



kable(cluster.count) %>%
  kable_styling(full_width=F)
```

#### Discussion 
Interesting the 3 cluster has substantially less 

```{r, warning=F}
replacenawith <- mean(df.members_Y1$age, na.rm=T)

df.plot <- df.members_Y1 
df.plot <- df.plot %>%
  mutate(cluster = km.out$cluster) %>%
  tidyr::replace_na(list(age=replacenawith))

df.members_Y1 <- df.members_Y1 %>%
  mutate(cluster = km.out$cluster) %>%
  tidyr::replace_na(list(age=replacenawith))


ggplot() +
   stat_density(
     data=df.plot, aes(x=age,colour=as.factor(cluster)),position="identity",geom="line") +
   stat_density(data=df.members_Y1, aes(x=age, colour='Population'), geom="line", size=1.5, color='black',lty=3) +
   scale_colour_discrete(name="Cluster") +
   labs(title='Comparison of Age Distribution', x='Age',y='Density')
```

Figure: ..


#### Discussion
```{r}
df.members <- df.members_Y1 %>%
  left_join(df.members, by='MemberID')

df.members_pop_clust <- df.members_Y1 %>%
  mutate(cluster='Population')

levels(df.members$Sex) <- c('Not Specified','Female','Male')

ggplot() +
      geom_bar(data=df.plot,   aes(x=as.factor(cluster),fill=df.members$Sex),position='dodge',stat="count")+
       geom_bar(data=df.members_pop_clust,aes(x=cluster,fill=df.members$Sex),position='dodge',stat="count") +
  scale_fill_discrete(name="Gender") +
  labs(x='Cluster',y='Count',title='Breakdown of Gender Across Clusters')
```

Figure:...

#### Discussion

### Heir

```{r}
df.members_Y1_h <- df.members_Y1_scaled %>%
  mutate(cluster = df.members_Y1$cluster) %>%
  filter(cluster==3) %>%
  select(male,female,Max_Charlson,Min_Charlson,char_diff)

d <- dist(df.members_Y1_h)

hc.complete <- hclust(d,method='complete')

hc.average <- hclust(d,method='average')

hc.single <- hclust(d,method='single')
```

```{r}
set.seed(33)
ks <- 2:25

ASW.complete <- sapply(ks, FUN=function(k) {
  cs <- fpc::cluster.stats(d, cutree(hc.complete, k))['avg.silwidth']
})

ASW.average <- sapply(ks, FUN=function(k) {
  cs <- fpc::cluster.stats(d, cutree(hc.average, k))['avg.silwidth']
})

ASW.single <- sapply(ks, FUN=function(k) {
  cs <- fpc::cluster.stats(d, cutree(hc.single, k))['avg.silwidth']
})


ASW.plot <- ks %>% as_tibble() %>%
  mutate(ASW.complete = unlist(ASW.complete),
         ASW.average = unlist(ASW.average),
         ASW.single = unlist(ASW.single))

ggplot(data=ASW.plot, aes(x=ks)) +
  geom_line(aes(y=ASW.complete, colour='Complete')) +
  geom_line(aes(y=ASW.average, colour='Average')) +
  geom_line(aes(y=ASW.single, colour='Single')) +
  labs(x='K',y='ASW') +
  scale_colour_discrete(name='Method')
```

Figure #:Comparison of methods for hierarchical clustering of Cluster 3

```{r}
hclust.counts <- summary(as.factor(cutree(hc.average,7)))

hclust.counts <- hclust.counts %>% as_tibble() %>%
  mutate(Cluster = 1:7) %>%
  dplyr::rename(N=value)

kable(hclust.counts) %>%
  kable_styling(full_width=F)
```



```{r}
dend.average <- as.dendrogram(hc.average)
plot(dend.average,main="Average",leaflab="none",ylab='Height')
abline(h=3.0634)
```

Figure #:....

#### Discussion

```{r}
df.hc.analysis <- df.members_Y1 %>% as_tibble() %>%
  filter(cluster==3) %>%
  mutate(cluster.dend = cutree(hc.average,7))

ggplot(df.hc.analysis, aes(colour=as.factor(cluster.dend),y=age)) +
  geom_boxplot() +
  labs(y='Age', x='Cluster') +
  scale_colour_discrete(name='cluster')
```


