---
output:
  html_document: default
  word_document: default
---


## Project 2
#### Group Members: Vince Miller & Sebastian Salomon

```{r setup, include=FALSE}
# create tool kit and visualization sizes
knitr::opts_chunk$set(fig.height=3.5,fig.width=8)
knitr::opts_chunk$set(echo = FALSE)
library('MASS')
library('dplyr')
library('knitr')
library('lemon')
library('tweedie')
library('kableExtra')
library('modeest')
library('ggplot2')
library('tidyr')
library('reshape')
library('egg')
library('gridExtra')
library('cowplot')
library('sqldf')
library('fpc')
library('akmeans')
library('stylo')
```


```{r echo=FALSE}
# Set working directory for 
setwd('C:\\Users\\MillerV\\Documents\\Masters Statistics\\CSE7331\\Projects\\DataMining_Projects')
## read in all the data and convert to tibble, this is the tidyverse dataframe
df.claims <- read.csv(file="Claims.csv") %>% as_tibble()
df.members <- read.csv(file="Members.csv") %>% as_tibble()
df.drugs <- read.csv(file="DrugCount.csv") %>% as_tibble()
df.labs <- read.csv(file="LabCount.csv") %>% as_tibble()
```
## 3. Data Preparation [30 points]
* Describe which features you want to use for clustering and why. [20]

```{r, echo=F}
# filter for year 1
df.claims_Y1 <- df.claims %>%
  filter(Year=='Y1') %>%
  select(-Year)

df.labs_Y1 <- df.labs %>%
  filter(Year=='Y1') %>%
  select(-Year)

df.drugs_Y1 <- df.drugs %>%
  filter(Year=='Y1') %>%
  select(-Year)
  
df.members_Y1 <- df.members

# free memory by removing unfiltered data
rm(df.claims,df.drugs,df.labs)

# create number of claims, providers, specialists, vendors, place service and pcp variables
df.members_Y1 <- df.claims_Y1 %>%
  group_by(MemberID) %>%
  summarize(N_claims = n(),
            N_Specialists=n_distinct(Specialty), 
            N_Providers=n_distinct(ProviderID),
            N_vendors=n_distinct(Vendor),
            N_PCP=n_distinct(PCP),
            N_places=n_distinct(PlaceSvc)) %>%
  left_join(df.members_Y1, by='MemberID')

# create number of drug claims variables assume 0 drug claims for member ID not in year 1 of drug dataset.
df.members_Y1 <- df.drugs_Y1 %>%
  group_by(MemberID) %>%
  summarize(N_drugclaims = n()) %>%
  right_join(df.members_Y1, by='MemberID') %>%
  mutate(N_drugclaims = replace_na(N_drugclaims,replace=0))

# create number of lab claims variables assume 0 drug claims for member ID not in year 1 of lab dataset.
df.members_Y1 <- df.labs_Y1 %>%
  group_by(MemberID) %>%
  summarize(N_labclaims = n()) %>%
  right_join(df.members_Y1, by='MemberID') %>%
  mutate(N_labclaims = replace_na(N_labclaims,replace=0))

# recode charleston index and create difference variable
df.members_Y1 <- df.claims_Y1 %>%
  mutate(CharlsonIndex = recode(CharlsonIndex, `0`=0, `1-2`=1.5, `3-4`=3.5, `5+`=5)) %>%
  group_by(MemberID) %>%
  summarize(Max_Charlson = max(CharlsonIndex),
            Min_Charlson = min(CharlsonIndex)) %>%
  mutate(char_diff = Max_Charlson - Min_Charlson) %>%
  right_join(df.members_Y1, by='MemberID')

# translate age variable and change sex to numeric
df.members_Y1 <- df.members_Y1 %>%
  mutate(age = recode(AgeAtFirstClaim, `""`=NULL, `0-9`=4.5, `10-19`=14.5, `20-29`=24.5, `30-39`=34.5, `40-49`=44.5, `50-59`=54.5, `60-69`= 64.5, `70-79`=74.5, `80+`=84.5),
         male = ifelse(Sex=='M',1,0),
         female= ifelse(Sex=='F',1,0)) %>%
  select(-Sex,-AgeAtFirstClaim)
```

```{r, echo=F}
# create table for viewing variables
df.created_features <- cbind(lapply(df.members_Y1, max,na.rm=T),lapply(df.members_Y1, mean, na.rm=T),lapply(df.members_Y1, min, na.rm=T),lapply(df.members_Y1,function(x) sum(is.na(x))))

# change col names and round
colnames(df.created_features) <- c('MAX','MEAN','MIN', 'NA Count')
df.created_features[2:15,] <- format(df.created_features[2:15,], digits=3)

# create table
kable(df.created_features[2:15,]) %>%
  group_rows('Charlson Features',1,3) %>% 
  group_rows('Counts',4,11) %>%
  group_rows('Age',12,12) %>% 
  group_rows('Gender',13,14) %>%
  kable_styling(full_width=F)

```
<center>Table xxx: Features used for clustering</center>

#### Discussion
All features for clustering were created at the member level. This was done to help understand how we can cluster patients as opposed to other possible levels such as claim level. 

The first 3 variables shown are the min, max, and difference for the charlson index for each of the patients claims. These variables can be used at an attempt to use severity of a members claim or the change of severity from claim to claim for a member.

Count variables were also created at the member level. The total number of claims by a member. The number of lab claims and drug claims each member made. The number of different specialist a member has seen. The number of unique providers, vendors, and pcp that each member has used for a claim as well as the number of different places. These count variables can be used to determine if patterns of activity can cluster our members.

The age of the members has been included. This variable can be used for controlling for age while clustering. Also it should be noted that Age has missing values. Our team chose to impute the values of age with the mean age for the entire members dataset. This could cause issues with our analysis because both kmeans and hierarchical clustering require each observation to be in a cluster. This imputation method could result in the clustering algorithm performance to degrage. Due to the reasons stated age will not be used for clustering but only for investigating the clusters. 

The gender of the member is also inclduded. This variable has been split into two columns; an indicator for male and an indicator for female. There were missing values therefore it is possible to have a 0 for each of these columns.

* What is the scale of measurement of the features and what are appropriate distance measures?[10]

```{r}
# standardize the data
df.members_Y1_scaled <- df.members_Y1 %>% 
  mutate_at(vars(-MemberID), funs(scale(.) %>% as.vector))
```

#### Discussion
The features used will be scaled to have a mean of 0 and a standard deviation of 1 for both kmeans, dbscan, and hierarchical clustering. Euclidean distance as opposed to cosine distance or a correlation measure will be used for each of the stated clustering methods. Currently, we do not have evidence to suggest one option of distance is better as opposed to others for our features. Additional analysis could be performed to compare how the clusters are formed using cosine distance or correlation measure, but that is not within the scope of our analysis. Clustering algorithms require subjective decisions such as distance measure choice.There is no doubt that differnt choices result in different clustering results, but due to the exploratory nature of clustering the ability to generalize intuitive results is a acceptable reasoning for selecting a distance. Our analysis does compare performance of single, average, and complete linkage when performing hierarchical clustering.

## 4. Modeling [50 points]
* Perform cluster analysis using several methods (at least k-means and hierarchical clustering)
using different feature subsets. [30]

#### Discussion
Our clustering analysis begins with kmeans. The variables selected for clustering were exclusively the count variables listed above. Our analysis plans to cluster members based on their activity. External validation is performed on the kmeans clusters using variables that were not including when clustering. Next, DBscan is used to cluster the dataset using the same count variables. Kmeans is comapred to DBscan using clustering comparison matrix. Lastly, hierarchical clustering is performed on what our group has concluded was the most interesting cluster found using kmeans.

### Kmeans

```{r, echo=F, warning=F}
# so the clusters are the same
set.seed(33)

# remove Member ID
df <- df.members_Y1_scaled %>% select(-MemberID)

# fill age
df <- df %>%
  mutate(age = replace_na(age, replace=0))

# create df for kmeans
# only include the count variables
df.kmeans <- df %>% select(N_labclaims,N_drugclaims,N_claims,N_Specialists,N_Providers,N_PCP,N_vendors,N_places)

# cluster
out.withinss <- rep(0,15)

for(i in seq(1,15,1)){
  out.withinss[i] <- kmeans(df.kmeans,i,nstart=20)$tot.withinss
  }

# plot the objective function kmeas attempts to minimize
plot(x=seq(1,15,1), y=out.withinss ,type="l", xlab='K', ylab='Total Within Sum Squares')
```

Figure xxx: Total Within Sum of Squares for Kmeans algorithmn applied to count variables

#### Discussion
Appears the objective function slope decreases after K=4. Our kmeans clustering analysis will examine 4 clusters.

```{r}
# from the plot above it appears at k=4 and k=5 there the slope decrease
km.out <- kmeans(df.kmeans,4,nstart=20)

def.par <- par(no.readonly = TRUE, mar = c(8.1, 4.1, 4.1, 2.1)) # save default, for resetting...
layout(t(1:4)) # 4 plots in one
for(i in 1:4) barplot(km.out$centers[i,], main=paste("Cluster", i),las=2, ylim=c(-2,4.5))
```



```{r}
cluster.count <- km.out$size %>% as_tibble() %>%
  mutate(Cluster = c(1,2,3,4)) %>%
  dplyr::rename(Count=value)



kable(cluster.count) %>%
  kable_styling(full_width=F)
```
<center> Table xxx: Count of members in each cluster </center>

#### Discussion
Examining the 4 clusters we can make judgements on the individuals within each group. Cluster 1 shows the average individual within our dataset related to amount of claims, lab claims, drug claims, specialists seen, etc. The team found it interesting that this group did not include the most members. Cluster 2 contains the most members. These individuals have approximately 1 standard deviation less than the average for each fo the count variables. These patients are individuals that could be assumed to not have chronic diseases. Cluster 2 contains good customers. Only an occasional claim. 

Cluster 3 contains individual's on average have approximately have 1 standard deviation more claims, .3 standard deviation more drug  claims, and .5 standard deviation more lab claims. We conclude that this group has less healthy individuals than cluster 1 and 2. The most interesting part of cluster 3 is the distinct primary care providers. These individuals have more primary care providers than each of the other three clusters. This extreme value only exists for the 3241 members in cluster 3. Later on cluster 3 will be investigated further using hierarchical clustering.

Cluster 4 contains unhealthy members similar to cluster 3, but they do not have many different primary care providers.
 

```{r, warning=F}
replacenawith <- mean(df.members_Y1$age, na.rm=T)

df.plot <- df.members_Y1 
df.plot <- df.plot %>%
  mutate(cluster = km.out$cluster) %>%
  tidyr::replace_na(list(age=replacenawith))

df.members_Y1 <- df.members_Y1 %>%
  mutate(cluster = km.out$cluster) %>%
  tidyr::replace_na(list(age=replacenawith))


ggplot() +
   stat_density(
     data=df.plot, aes(x=age,colour=as.factor(cluster)),position="identity",geom="line") +
   stat_density(data=df.members_Y1, aes(x=age, colour='Population'), geom="line", size=1.5, color='black',lty=3) +
   scale_colour_discrete(name="Cluster") +
   labs(title='Comparison of Age Distribution', x='Age',y='Density')
```

Figure xxx: Comparison of age density for each cluster as well as the population of the entire dataset shown in black

#### Discussion
Cluster 1 has a distribution that is slightly older than the true population of the dataset. This is interesting because if you remember we stated that we suspect that these are the average members within the dataset. A statistical test would need to be performed to determine if cluster 1 has an older skewed distribution. Cluster 2 contains on average the younger members within the dataset. This could have been expected from the cluster because we noted in cluster 2 that the individuals were probably the healthiest. Cluster 3 is interesting because the weight of the distribution is disproportionally weighted at ~50 years old. Our team suggested that the members in cluster 4 were probably the most unhealthy therefore it is no suprise the density is concentrated at 75 years old.

Cluster 3 and Cluster 4 both include on average the more unhealthy individuals, but cluster 3 contained members who had relitively more primary care providers. With new information suggesting cluster 3 is younger it is possible that after the age of 50 individuals are less likely to change their primary care provider. 

```{r}
df.members <- df.members_Y1 %>%
  left_join(df.members, by='MemberID')

df.members_pop_clust <- df.members_Y1 %>%
  mutate(cluster='Population')

levels(df.members$Sex) <- c('Not Specified','Female','Male')

ggplot() +
      geom_bar(data=df.plot,   aes(x=as.factor(cluster),fill=df.members$Sex),position='dodge',stat="count")+
       geom_bar(data=df.members_pop_clust,aes(x=cluster,fill=df.members$Sex),position='dodge',stat="count") +
  scale_fill_discrete(name="Gender") +
  labs(x='Cluster',y='Count',title='Breakdown of Gender Across Clusters')
```

Figure xxx: Age breakdown for each of the kmeans clusters as well as population

#### Discussion
Once again cluster 3 is the most interesting. Cluster 3 distribution related to gender has a much higher count of missing values for gender. An investigation as to why this has occured could show that we have a severe data quality issue for the individuals that have more than 1 primary care provider.

### Hierarchical

```{r}
df.members_Y1_h <- df.members_Y1_scaled %>%
  mutate(cluster = df.members_Y1$cluster) %>%
  filter(cluster==3) %>%
  select(male,female,Max_Charlson,Min_Charlson,char_diff)

d <- dist(df.members_Y1_h)

hc.complete <- hclust(d,method='complete')

hc.average <- hclust(d,method='average')

hc.single <- hclust(d,method='single')
```

```{r}
set.seed(33)
ks <- 2:25

ASW.complete <- sapply(ks, FUN=function(k) {
  cs <- fpc::cluster.stats(d, cutree(hc.complete, k))['avg.silwidth']
})

ASW.average <- sapply(ks, FUN=function(k) {
  cs <- fpc::cluster.stats(d, cutree(hc.average, k))['avg.silwidth']
})

ASW.single <- sapply(ks, FUN=function(k) {
  cs <- fpc::cluster.stats(d, cutree(hc.single, k))['avg.silwidth']
})


ASW.plot <- ks %>% as_tibble() %>%
  mutate(ASW.complete = unlist(ASW.complete),
         ASW.average = unlist(ASW.average),
         ASW.single = unlist(ASW.single))

ggplot(data=ASW.plot, aes(x=ks)) +
  geom_line(aes(y=ASW.complete, colour='Complete')) +
  geom_line(aes(y=ASW.average, colour='Average')) +
  geom_line(aes(y=ASW.single, colour='Single')) +
  labs(x='K',y='ASW') +
  scale_colour_discrete(name='Method')
```

Figure xxx:Comparison of methods for hierarchical clustering of Cluster 3

```{r}
hclust.counts <- summary(as.factor(cutree(hc.average,7)))

hclust.counts <- hclust.counts %>% as_tibble() %>%
  mutate(Cluster = 1:7) %>%
  dplyr::rename(N=value)

kable(hclust.counts) %>%
  kable_styling(full_width=F)
```

<center> Table xxx: Count for each of the 7 clusters

#### Discussion
The figure shown above is a comparison of average silouette width for the different methods possible in the hierarchical clustering algorithmn when clustering exclusively the 3 cluster found with the kmeans algorithmn.

The features used for clustering were the gender features as well as the charlson features that were shown in the table earlier.

The plot showed puzzling results. Since no peak was found within 15 clusters the plot was extended to show that splitting the data into smaller and smaller clusters continues to improve the results. Our team was concerned that with clusters greater than 10 some clusters would contain too few observations. This is an issue with hierarchical clustering. Each observation must be assigned a cluster. An outlier analysis could potentially show that a few observations are causing the clustering algorithmn to give these unexpected results. Eventually, our team decided against an outlier analysis and decided to investigate 7 clusters using the average linkage method.


```{r}
dend.average <- as.dendrogram(hc.average)
plot(dend.average,main="Average",leaflab="none",ylab='Height')
abline(h=2.8634)
```

Figure xxx: Dendrogram of the heirarchical clustering perfomred on cluster 3 for kmeans. The horizontal line represents the height at which the tree was cut to form 7 clusters.

#### Discussion
The dendrogram shows the first 3 clusters are very different in terms of height from the last 4. 

```{r}
df.hc.analysis <- df.members_Y1 %>% as_tibble() %>%
  filter(cluster==3) %>%
  mutate(cluster.dend = cutree(hc.average,7))

ggplot(df.hc.analysis, aes(colour=as.factor(cluster.dend),y=age)) +
  geom_boxplot() +
  labs(y='Age', x='Cluster') +
  scale_colour_discrete(name='cluster') +
  theme(axis.text.x=element_blank())
```
Figure xxx: 

#### Discussion
Using age to examine heirarchical clustering of cluster 3 from kmeans shows interesting results. The first 3 clusters have a full range of age for the data set while the last 4 clusters have only older members. Also, the first cluster contains mainly 35-55 year old individuals. This is interesting because earlier we found a disproportional representation of this age group within this cluster. A hospital could be interested in what is different between these clusters formed with heirarchical clustering to determine who are the members that make cluster 3 from the kmeans analysis so interesting. 
